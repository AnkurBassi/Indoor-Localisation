{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Go to Google Drive -> Click \"Shared with me\" (left nativation bar) -> Find the folder \"indoor-location-competition\" -> Right Click the folder -> Click Organize->\"Add shortcut to Drive\" -> Select \"My Drive\" and Click \"ADD SHORTCUT\" > A window pops->go to All location and select->MyDrive\n",
        "\n",
        "https://drive.google.com/drive/folders/1--XrdfzFElOb1gPBv_aeMe1doX_Yo2WN?usp=drive_link"
      ],
      "metadata": {
        "id": "QTXZ3O9-yeg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Do you want to give default settings for hyperparameters or custom\")\n",
        "inp=input(\"Enter 1 for default and 2 for custom\")\n",
        "if int(inp)==2:\n",
        "  wifi_embd=int(input(\"Embeddings dimension for wifi\"))\n",
        "  wifi_out_dimension=int(input(\"Wifi signal out dimensions\"))\n",
        "  nhead=int(input(\"Enter number of attention heads\"))\n",
        "  nhid=int(input(\"Enter number of neurons for fully connected transformer layers\"))\n",
        "  dropout=float(input(\"dropout percentage\"))\n",
        "  Epochs=int(input(\"Number of Epochs\"))\n",
        "\n",
        "else:\n",
        "  wifi_embd=128\n",
        "  wifi_out_dimension=1280\n",
        "  nhead=2\n",
        "  nhid=512\n",
        "  dropout=0.1\n",
        "  Epochs=400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIEopXnXw3gR",
        "outputId": "b55eb83a-9bbd-4982-9646-f9d01b20863f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to give default settings for hyperparameters or custom\n",
            "Enter 1 for default and 2 for custom1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjAmaD6Agpx8",
        "outputId": "181fcb34-d415-46cf-c1cf-d5c55a214db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8zuuuusxgvjo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os, random, pickle, gc, time, json, math, copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.utils import rnn\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fd2MNFMLgyT_"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZVN3jiqFg382"
      },
      "outputs": [],
      "source": [
        "dat_ver = 'ILN_631dat'\n",
        "model_ver = 'ILN_632'\n",
        "subm_file = 'sample_submission.csv'\n",
        "\n",
        "N_SPLITS = 10\n",
        "NUM_FEATS = 40\n",
        "SEED = 1605\n",
        "batch_size = 128\n",
        "inference_only = True\n",
        "\n",
        "if inference_only:\n",
        "    n_epoch = 1\n",
        "    data_dir = 'ILN_train_results'\n",
        "else:\n",
        "    n_epoch = 2000\n",
        "    data_dir = ''\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "ID_FEATS = [f'id_{i}' for i in range(NUM_FEATS)]\n",
        "STRG_FEATS  = [f'strength_{i}' for i in range(NUM_FEATS)]\n",
        "IMU_FEATS = ['gyro_x_mean', 'gyro_y_mean', 'gyro_z_mean',\n",
        "             'gyro_x_std', 'gyro_y_std', 'gyro_z_std',\n",
        "             'gyro_x_max', 'gyro_y_max', 'gyro_z_max',\n",
        "             'gyro_x_min', 'gyro_y_min', 'gyro_z_min',\n",
        "             'gyro_x_skew', 'gyro_y_skew', 'gyro_z_skew',\n",
        "             'acce_x_mean', 'acce_y_mean', 'acce_z_mean',\n",
        "             'acce_x_std', 'acce_y_std', 'acce_z_std',\n",
        "             'acce_x_max', 'acce_y_max', 'acce_z_max',\n",
        "             'acce_x_min', 'acce_y_min', 'acce_z_min',\n",
        "             'acce_x_skew', 'acce_y_skew', 'acce_z_skew',\n",
        "             'ahrs_x_mean', 'ahrs_y_mean', 'ahrs_z_mean',\n",
        "             'ahrs_x_std', 'ahrs_y_std', 'ahrs_z_std',\n",
        "             'ahrs_x_max', 'ahrs_y_max', 'ahrs_z_max',\n",
        "             'ahrs_x_min', 'ahrs_y_min', 'ahrs_z_min',\n",
        "             'ahrs_x_skew', 'ahrs_y_skew', 'ahrs_z_skew',\n",
        "             'head_magn_x_mean', 'head_magn_y_mean',\n",
        "             'head_magn_x_std', 'head_magn_y_std',\n",
        "             'head_magn_x_max', 'head_magn_y_max',\n",
        "             'head_magn_x_min', 'head_magn_y_min',\n",
        "             'head_magn_x_skew', 'head_magn_y_skew',\n",
        "             'magn_z_mean', 'magn_z_std',\n",
        "             'magn_z_max', 'magn_z_min', 'magn_z_skew']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kS_mN6m0g7s1"
      },
      "outputs": [],
      "source": [
        "floor_map = {\"B2\":-2, \"B1\":-1,\n",
        "             \"F1\":0, \"F2\":1, \"F3\":2, \"F4\":3, \"F5\":4,\n",
        "             \"F6\":5, \"F7\":6, \"F8\":7, \"F9\":8,\n",
        "             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4,\n",
        "             \"6F\":5, \"7F\":6, \"8F\":7, \"9F\":8}\n",
        "site_list = ['5d2709a003f801723c3251bf','5a0546857ecc773753327266',\n",
        "             '5c3c44b80379370013e0fd2b','5d2709b303f801723c327472',\n",
        "             '5d2709bb03f801723c32852c','5d2709c303f801723c3299ee',\n",
        "             '5d2709d403f801723c32bd39','5d2709e003f801723c32d896',\n",
        "             '5d27075f03f801723c2e360f','5d27096c03f801723c31e5e0',\n",
        "             '5d27097f03f801723c320d97','5d27099f03f801723c32511d',\n",
        "             '5da138b74db8ce0c98bd4774','5da958dd46f8266d0737457b',\n",
        "             '5da1382d4db8ce0c98bbe92e','5da1383b4db8ce0c98bc11ab',\n",
        "             '5da1389e4db8ce0c98bd0547','5da138274db8ce0c98bbd3d2',\n",
        "             '5da138314db8ce0c98bbf3a0','5da138364db8ce0c98bc00f1',\n",
        "             '5da138754db8ce0c98bca82f','5da138764db8ce0c98bcaa46',\n",
        "             '5dbc1d84c1eb61796cf7c010','5dc8cea7659e181adb076a3f']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5LGMOkx9dYNL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pickle, gc, glob\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1xYK1VIYfh3R"
      },
      "outputs": [],
      "source": [
        "''' Retrieve the Site ID from txt file '''\n",
        "def SiteID(txt):\n",
        "    p1 = txt[1].find('SiteID:')+7\n",
        "    p2 = txt[1].find('\\tSiteName:')\n",
        "    assert p1!=-1+7 and p2!=-1, 'SiteID not found'\n",
        "    return txt[1][p1:p2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLFLgzQhhLqI",
        "outputId": "70506481-af68-4d76-ed0b-00a94a00c2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed '/content/indoor-location-competition'\n",
            "'/content/drive/MyDrive/indoor-location-competition' -> '/content/indoor-location-competition'\n"
          ]
        }
      ],
      "source": [
        "cp -var /content/drive/MyDrive/indoor-location-competition /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFOKtmKqrSyu",
        "outputId": "ca48e105-7f35-4811-c088-31e2b54ee7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-84-4590d0f29efa>:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-deep')\n",
            "<ipython-input-84-4590d0f29efa>:9: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-darkgrid')\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc, glob, time, pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-deep')\n",
        "plt.style.use('seaborn-darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DrytVGm0r49T"
      },
      "outputs": [],
      "source": [
        "input_dir = './'\n",
        "\n",
        "with open('test_site_dict.pkl', 'rb') as f:\n",
        "    test_site_dict = pickle.load(f)\n",
        "\n",
        "floor_map = {\"B2\":-2, \"B1\":-1,\n",
        "             \"F1\":0, \"F2\":1, \"F3\":2, \"F4\":3, \"F5\":4,\n",
        "             \"F6\":5, \"F7\":6, \"F8\":7, \"F9\":8,\n",
        "             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4,\n",
        "             \"6F\":5, \"7F\":6, \"8F\":7, \"9F\":8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "St9ErLgEsGMh"
      },
      "outputs": [],
      "source": [
        "''' Retrieve the Site ID from txt file '''\n",
        "def SiteID(txt):\n",
        "    p1 = txt[1].find('SiteID:')+7\n",
        "    p2 = txt[1].find('\\tSiteName:')\n",
        "    assert p1!=-1+7 and p2!=-1, 'SiteID not found'\n",
        "    return txt[1][p1:p2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AHfTRutgslKD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Modify the host's code \"read_data_file\" function in \"io_f.py\"\n",
        "for dealing with the malformed data etc.\n",
        "'''\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ReadData:\n",
        "    acce: np.ndarray\n",
        "    acce_uncali: np.ndarray\n",
        "    gyro: np.ndarray\n",
        "    gyro_uncali: np.ndarray\n",
        "    magn: np.ndarray\n",
        "    magn_uncali: np.ndarray\n",
        "    ahrs: np.ndarray\n",
        "    wifi: np.ndarray\n",
        "    ibeacon: np.ndarray\n",
        "    waypoint: np.ndarray\n",
        "\n",
        "def split_list_as_req(line_data):\n",
        "    redo = False\n",
        "    data_BU = []\n",
        "    header_list = [i for i, itm in enumerate(line_data) if 'TYPE_' in itm]\n",
        "    if len(header_list) > 1:\n",
        "        data_BU = [line_data[header_list[1]-1][-13:]] + line_data[header_list[1]:]\n",
        "        line_data[header_list[1]-1] = line_data[header_list[1]-1][:-13]\n",
        "        line_data = line_data[:header_list[1]]\n",
        "        redo = True\n",
        "    return redo, line_data, data_BU\n",
        "\n",
        "def read_data_file(data_filename):\n",
        "    acce = []\n",
        "    acce_uncali = []\n",
        "    gyro = []\n",
        "    gyro_uncali = []\n",
        "    magn = []\n",
        "    magn_uncali = []\n",
        "    ahrs = []\n",
        "    wifi = []\n",
        "    ibeacon = []\n",
        "    waypoint = []\n",
        "\n",
        "    with open(data_filename, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    i = 0\n",
        "    redo = False\n",
        "    while i < len(lines):\n",
        "        if not redo:\n",
        "            line_data = lines[i]\n",
        "            line_data = line_data.strip()\n",
        "            if not line_data or line_data[0] == '#':\n",
        "                i += 1\n",
        "                continue\n",
        "            line_data = line_data.split('\\t')\n",
        "        else:\n",
        "            line_data = data_BU\n",
        "            redo = False\n",
        "\n",
        "        redo, line_data, data_BU = split_list_as_req(line_data)\n",
        "\n",
        "        if line_data[1] == 'TYPE_ACCELEROMETER':\n",
        "            try:\n",
        "                acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "            except ValueError:\n",
        "                print(data_filename)\n",
        "                print(line_data)\n",
        "\n",
        "        elif line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n",
        "            acce_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_GYROSCOPE':\n",
        "            gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n",
        "            gyro_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_MAGNETIC_FIELD':\n",
        "            magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n",
        "            magn_uncali.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_ROTATION_VECTOR':\n",
        "            ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n",
        "\n",
        "        elif line_data[1] == 'TYPE_WIFI':\n",
        "            sys_ts = line_data[0]\n",
        "            ssid = line_data[2]\n",
        "            bssid = line_data[3]\n",
        "            rssi = line_data[4]\n",
        "            lastseen_ts = line_data[6]\n",
        "            frequency = line_data[5]\n",
        "            wifi_data = [sys_ts, ssid, bssid, rssi, lastseen_ts, frequency]\n",
        "            wifi.append(wifi_data)\n",
        "\n",
        "        elif line_data[1] == 'TYPE_BEACON':\n",
        "            ts = line_data[0]\n",
        "            uuid = line_data[2]\n",
        "            major = line_data[3]\n",
        "            minor = line_data[4]\n",
        "            rssi = line_data[6]\n",
        "            txpow = line_data[5]\n",
        "            distance = line_data[7]\n",
        "            mac = line_data[8]\n",
        "            if len(line_data)>9:\n",
        "                ts_copy = line_data[9]\n",
        "            else:\n",
        "                ts_copy = ts\n",
        "            ibeacon_data = [ts, '_'.join([uuid, major, minor]), rssi,\n",
        "                            txpow, distance, mac, ts_copy]\n",
        "            ibeacon.append(ibeacon_data)\n",
        "\n",
        "        elif line_data[1] == 'TYPE_WAYPOINT':\n",
        "            waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n",
        "\n",
        "        if not redo:\n",
        "            i += 1\n",
        "\n",
        "    acce = np.array(acce)\n",
        "    acce_uncali = np.array(acce_uncali)\n",
        "    gyro = np.array(gyro)\n",
        "    gyro_uncali = np.array(gyro_uncali)\n",
        "    magn = np.array(magn)\n",
        "    magn_uncali = np.array(magn_uncali)\n",
        "    ahrs = np.array(ahrs)\n",
        "    wifi = np.array(wifi)\n",
        "    ibeacon = np.array(ibeacon)\n",
        "    waypoint = np.array(waypoint)\n",
        "\n",
        "    return ReadData(acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iyiHltC-yPBG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import gc, glob, time, pickle, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from visualize_f import visualize_trajectory\n",
        "import compute_f as F\n",
        "from scipy.interpolate import interp1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SvPjupbp0ZiW"
      },
      "outputs": [],
      "source": [
        "file_header = 'ILN_631dat'\n",
        "input_dir = './'\n",
        "floor_map = {\"B2\":-2, \"B1\":-1,\n",
        "             \"F1\":0, \"F2\":1, \"F3\":2, \"F4\":3, \"F5\":4,\n",
        "             \"F6\":5, \"F7\":6, \"F8\":7, \"F9\":8,\n",
        "             \"1F\":0, \"2F\":1, \"3F\":2, \"4F\":3, \"5F\":4,\n",
        "             \"6F\":5, \"7F\":6, \"8F\":7, \"9F\":8}\n",
        "site_list = ['5d2709a003f801723c3251bf','5a0546857ecc773753327266',\n",
        "             '5c3c44b80379370013e0fd2b','5d2709b303f801723c327472',\n",
        "             '5d2709bb03f801723c32852c','5d2709c303f801723c3299ee',\n",
        "             '5d2709d403f801723c32bd39','5d2709e003f801723c32d896',\n",
        "             '5d27075f03f801723c2e360f','5d27096c03f801723c31e5e0',\n",
        "             '5d27097f03f801723c320d97','5d27099f03f801723c32511d',\n",
        "             '5da138b74db8ce0c98bd4774','5da958dd46f8266d0737457b',\n",
        "             '5da1382d4db8ce0c98bbe92e','5da1383b4db8ce0c98bc11ab',\n",
        "             '5da1389e4db8ce0c98bd0547','5da138274db8ce0c98bbd3d2',\n",
        "             '5da138314db8ce0c98bbf3a0','5da138364db8ce0c98bc00f1',\n",
        "             '5da138754db8ce0c98bca82f','5da138764db8ce0c98bcaa46',\n",
        "             '5dbc1d84c1eb61796cf7c010','5dc8cea7659e181adb076a3f']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mLZgv6Dc0hBE"
      },
      "outputs": [],
      "source": [
        "with open(\"test_site_dict.pkl\", \"rb\") as f:\n",
        "    test_site_dict = pickle.load(f)\n",
        "df_TestTimeLag = pd.read_csv('test_ts_lag.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J6DF9wFZ0qML"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create sequence of time & relative position data for each path\n",
        "Outputs are as follows:\n",
        "  PathSeq_t: timestamp\n",
        "  PathSeq_l: length position (cumulative summation (cumsum) of walking distance)\n",
        "             at each timestamp\n",
        "  PathSeq_rx, PathSeq_ry: relative position\n",
        "'''\n",
        "def Path_Sequence(PathData):\n",
        "    step_timestamps, step_indexs, step_acce_max_mins = \\\n",
        "        F.compute_steps(PathData.acce)\n",
        "    headings = F.compute_headings(PathData.ahrs)\n",
        "    stride_lengths = F.compute_stride_length(step_acce_max_mins)\n",
        "    step_headings = F.compute_step_heading(step_timestamps, headings)\n",
        "    PathSeq_t = stride_lengths[:,0]\n",
        "    PathSeq_t = np.insert(PathSeq_t, 0,\n",
        "                          PathData.acce[0,0]).astype('int64')\n",
        "    PathSeq_l = stride_lengths[:,1].cumsum()\n",
        "    PathSeq_l = np.append(np.array(0),PathSeq_l)\n",
        "\n",
        "    rel_pos = F.compute_rel_positions(stride_lengths, step_headings)\n",
        "    PathSeq_rx = np.append(np.array(0),rel_pos[:,1]).cumsum()\n",
        "    PathSeq_ry = np.append(np.array(0),rel_pos[:,2]).cumsum()\n",
        "    return PathSeq_t, PathSeq_l, PathSeq_rx, PathSeq_ry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8HXzpdDW0uNH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create pd.DataFrame of wifi signal data for each path\n",
        "Basic format is the same as that created with Kouki's great notebook.\n",
        "(https://www.kaggle.com/kokitanisaka/create-unified-wifi-features-example)\n",
        "- rows: timestamp (or corresponding Length position)\n",
        "- columns: signal id and strength, which are sorted by strength at the same timestamp\n",
        "'''\n",
        "def dfSignalSequence(df_sign, ref_time, id_list,\n",
        "                     id_, values, tslabel, n_in_seq):\n",
        "\n",
        "    ''' Drop the data out of \"tslabel\" range '''\n",
        "    mask = df_sign[id_].apply(lambda x: x in id_list)\n",
        "    mask &= (df_sign[tslabel] >= ref_time[0])\n",
        "    mask &= (df_sign[tslabel] <= ref_time[-1])\n",
        "    df_sign = df_sign[mask]\n",
        "    if df_sign.shape[0]==0:\n",
        "        return\n",
        "\n",
        "    ''' Drop the data before the previous timestamp '''\n",
        "    df_tmp = pd.DataFrame(df_sign[tslabel].unique(),columns=[tslabel])\n",
        "    df_tmp['last_ts'] = np.append(ref_time[0],\n",
        "                                  df_tmp[tslabel].values[:-1])\n",
        "    df_sign = df_sign.merge(df_tmp, how='left', on=tslabel)\n",
        "    del df_tmp; gc.collect()\n",
        "    df_sign = df_sign[df_sign['lastseen_ts']>=df_sign['last_ts']]\n",
        "    df_sign.drop('last_ts', axis=1, inplace=True)\n",
        "    if df_sign.shape[0]==0:\n",
        "        return\n",
        "\n",
        "    ''' create pivot table '''\n",
        "    df_sign.set_index(tslabel, inplace=True)\n",
        "    df_pivot = df_sign.pivot_table(index=df_sign.index,\n",
        "                                   columns=id_, values=values,\n",
        "                                   aggfunc=max)\n",
        "    feat = []\n",
        "    for i in range(df_pivot.shape[0]):\n",
        "        tmp = df_pivot.iloc[i,:].sort_values(ascending=False)[:n_in_seq]\n",
        "        if tmp.shape[0]!=n_in_seq:\n",
        "            n_col = tmp.shape[0]\n",
        "            add_col = list(set(id_list)-set(tmp.index))[:n_in_seq-n_col]\n",
        "            tmp = pd.concat([tmp, pd.Series(np.nan, index=add_col)])\n",
        "            assert tmp.shape[0]==n_in_seq\n",
        "        tmp = tmp.reset_index().values.T.reshape(-1)\n",
        "        feat.append(tmp)\n",
        "    feat = np.stack(feat,axis=1).T\n",
        "    df_out = pd.DataFrame(feat,\n",
        "                          columns=[f'id_{i}' for i in range(n_in_seq)]\n",
        "                                 +[f'strength_{i}' for i in range(n_in_seq)])\n",
        "    df_out[tslabel] = df_pivot.index\n",
        "    df_out.set_index(tslabel, drop=True, inplace=True)\n",
        "    return df_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IHSGY2lH0yrI"
      },
      "outputs": [],
      "source": [
        "''' Retrieve the Site ID from txt file '''\n",
        "def SiteID(txt):\n",
        "    p1 = txt[1].find('SiteID:')+7\n",
        "    p2 = txt[1].find('\\tSiteName:')\n",
        "    assert p1!=-1+7 and p2!=-1, 'SiteID not found'\n",
        "    return txt[1][p1:p2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IhnxGZn101Mq"
      },
      "outputs": [],
      "source": [
        "def headings_from_magn(PathData):\n",
        "    mag_df = pd.DataFrame(PathData.magn)\n",
        "    mag_df.columns = [\"timestamp\",\"x\",\"y\",\"z\"]\n",
        "    acce_df = pd.DataFrame(PathData.acce)\n",
        "    acce_df.columns = [\"timestamp\",\"ax\",\"ay\",\"az\"]\n",
        "    mag_df = pd.merge(mag_df,acce_df,on=\"timestamp\").dropna()\n",
        "\n",
        "    m_trans = 0\n",
        "    time_di_list = []\n",
        "    for i in mag_df.iterrows():\n",
        "\n",
        "        ''' https://www.kaggle.com/museas/with-magn-cost-minimization '''\n",
        "        gx,gy,gz = i[1][1],i[1][2],i[1][3]\n",
        "        ax,ay,az = i[1][4],i[1][5],i[1][6]\n",
        "        roll = math.atan2(ay,az)\n",
        "        pitch = math.atan2(-1*ax , (ay * math.sin(roll) + az * math.cos(roll)))\n",
        "        q = m_trans - math.atan2(\n",
        "            (gz*math.sin(roll)-gy*math.cos(roll)),(gx*math.cos(pitch) + gy*math.sin(roll)*math.sin(pitch) + gz*math.sin(pitch)*math.cos(roll))\n",
        "        ) -np.pi/2\n",
        "        q = (q+np.pi)%(2*np.pi)-np.pi\n",
        "\n",
        "        ''' The following is a different calculation for verification '''\n",
        "#         mag, acce = i[1][1:4].values, i[1][4:7].values\n",
        "#         axsz = acce/np.linalg.norm(acce)\n",
        "#         axsy = mag/np.linalg.norm(mag)\n",
        "#         axsy -= np.sum(axsy*axsz) * axsz\n",
        "#         axsy = axsy/np.linalg.norm(axsy)\n",
        "#         axsx = np.cross(axsy, axsz)\n",
        "#         q1 = np.arctan2(axsx[1],axsy[1])\n",
        "#         q1 = (q1+np.pi)%(2*np.pi)-np.pi\n",
        "\n",
        "        time_di_list.append((i[1][0],q))\n",
        "    return np.array(time_di_list)\n",
        "\n",
        "def add_IMU(PathData, df_inpt, time_delta=0):\n",
        "\n",
        "    headmagn = headings_from_magn(PathData)\n",
        "    df_IMU = pd.DataFrame(PathData.gyro,columns=['IMU_ts','gyro_x','gyro_y','gyro_z'])\n",
        "    df_IMU['IMU_ts'] = (df_IMU['IMU_ts']+time_delta).astype('int64')\n",
        "#     assert (df_IMU['IMU_ts'].values==headmagn[:,0]).all()\n",
        "#     assert (df_IMU['IMU_ts'].values==PathData.acce[:,0]).all()\n",
        "#     assert (df_IMU['IMU_ts'].values==PathData.ahrs[:,0]).all()\n",
        "    df_IMU['head_magn_x'] = np.sin(headmagn[:,1])\n",
        "    df_IMU['head_magn_y'] = np.cos(headmagn[:,1])\n",
        "    df_IMU[['acce_x','acce_y','acce_z']] = PathData.acce[:,1:]\n",
        "    df_IMU[['ahrs_x','ahrs_y','ahrs_z']] = PathData.ahrs[:,1:]\n",
        "    df_IMU[['magn_x','magn_y','magn_z']] = PathData.magn[:,1:]\n",
        "\n",
        "    ''' set window label '''\n",
        "    grp_ts = df_inpt.index.name\n",
        "    df_IMU = pd.concat([pd.DataFrame([df_inpt.index]*2,index=['IMU_ts',grp_ts]).T,\n",
        "                        df_IMU], axis=0)\n",
        "    df_IMU = df_IMU.sort_values('IMU_ts')\n",
        "    df_IMU[grp_ts].fillna(method='bfill',inplace=True)\n",
        "    df_IMU.dropna(inplace=True)\n",
        "    df_IMU[grp_ts] = df_IMU[grp_ts].astype('int64')\n",
        "    df_IMU.drop('IMU_ts', axis=1, inplace=True)\n",
        "\n",
        "    ''' grouping by timestamp of wifi data '''\n",
        "    gdf_IMU = pd.concat([df_IMU.groupby(grp_ts).mean().add_suffix('_mean'),\n",
        "                         df_IMU.groupby(grp_ts).std().add_suffix('_std'),\n",
        "                         df_IMU.groupby(grp_ts).max().add_suffix('_max'),\n",
        "                         df_IMU.groupby(grp_ts).min().add_suffix('_min'),\n",
        "                         df_IMU.groupby(grp_ts).skew().add_suffix('_skew')],axis=1)\n",
        "\n",
        "    return df_inpt.merge(gdf_IMU, how='left', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cfe7jQku07U-"
      },
      "outputs": [],
      "source": [
        "tmp_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x472wCDs1Fcx"
      },
      "outputs": [],
      "source": [
        "df_TestTimeLag = pd.read_csv('test_ts_lag.csv',index_col=0)\n",
        "with open(f'{dat_ver}_df_wifi_all.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHgICHM68FkY",
        "outputId": "9527944c-15be-4c39-ef3c-de8a0f139cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████| 11374/11374 [00:19<00:00, 593.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total num of path data: 277772\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "- Cutoff based on signal strength\n",
        "  If \"thresh\" is set less than or equals to -100, cutoff is not performed.\n",
        "- Divide long sequence into short ones (maximum length is max_seq_len)\n",
        "  by GPU memory limitation.\n",
        "'''\n",
        "min_seq_len = 10\n",
        "max_seq_len = 200\n",
        "tmp = []\n",
        "for gid, grp in tqdm(data.groupby('path'), ncols=60):\n",
        "    if grp.shape[0] > min_seq_len:\n",
        "        thresh = np.sort(grp['strength_0'].values)[-min_seq_len]\n",
        "        thresh = min([-100, thresh])\n",
        "        grp = grp[grp['strength_0']>=thresh].copy()\n",
        "    grp['seq_label'] = grp['path'].str.cat((np.arange(grp.shape[0])\n",
        "                                            //max_seq_len).astype(str),\n",
        "                                           sep='_')\n",
        "    tmp.append(grp)\n",
        "data = pd.concat(tmp)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "del tmp; gc.collect()\n",
        "print(f'total num of path data: {data.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "waiO38-_-Rj6"
      },
      "outputs": [],
      "source": [
        "tslabel ='sys_ts'\n",
        "data['site_path_timestamp'] = (data['site']\n",
        "                               .str.cat(data['path'], sep='_'))\n",
        "# timestamp is subtract with time lag value (test data only)\n",
        "mask = data['train/test']=='train'\n",
        "data.loc[ mask,'timestamp'] = data.loc[mask,tslabel]\n",
        "data.loc[~mask,'timestamp'] = (data.loc[~mask,tslabel]\n",
        "                              -df_TestTimeLag.loc[data.loc[~mask,'path']]\n",
        "                              .values.reshape(-1))\n",
        "data['site_path_timestamp'] = (data['site_path_timestamp']\n",
        "                               .str.cat(data['timestamp']\n",
        "                                        .astype('int64').astype(str)\n",
        "                                        .str.zfill(13), sep='_'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zLmpnoGp-FQq"
      },
      "outputs": [],
      "source": [
        "OTHER_FEATS = [itm for itm in data.columns\n",
        "               if (itm[:3]!='id_')&(itm[:9]!='strength_')&\n",
        "                  (itm[-5:]!='_mean')&(itm[-4:]!='_std')&\n",
        "                  (itm[-4:]!='_max')&(itm[-4:]!='_min')&\n",
        "                  (itm[-5:]!='_skew')]\n",
        "data = data[OTHER_FEATS+ID_FEATS+STRG_FEATS+IMU_FEATS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBPDKVcbWAMX",
        "outputId": "72d00a00-84d6-4867-c4cf-607b6e6318e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# data[OTHER_FEATS+ID_FEATS+STRG_FEATS+IMU_FEATS]\n",
        "len(OTHER_FEATS+ID_FEATS+STRG_FEATS+IMU_FEATS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ2v1nyS-ZyM",
        "outputId": "bb8bed6c-38c2-49f7-9399-5a7f705b2a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID vocabulary: 36646\n"
          ]
        }
      ],
      "source": [
        "# signal ID vocabulary for embedding layer\n",
        "ID_vocab = sorted(list(set(data[ID_FEATS].values.reshape(-1))))\n",
        "print(f'ID vocabulary: {len(ID_vocab)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU7VIk4kAy-u",
        "outputId": "a4b1a99b-02a6-4bd7-d092-326b56214baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
            "<ipython-input-33-8e62589c0744>:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:, 'site'] = le_site.transform(data.loc[:, 'site'])\n"
          ]
        }
      ],
      "source": [
        "''' label encoding '''\n",
        "le_id = LabelEncoder()\n",
        "le_id.fit(ID_vocab)\n",
        "le_site = LabelEncoder()\n",
        "le_site.fit(data['site'].unique())\n",
        "for i in ID_FEATS:\n",
        "    data.loc[:,i] = le_id.transform(data.loc[:,i])+1\n",
        "data.loc[:, 'site'] = le_site.transform(data.loc[:, 'site'])\n",
        "\n",
        "# strength=nan -> token=0 (for zero padding)\n",
        "data[ID_FEATS] *= (~(data[STRG_FEATS].isna()).values)\n",
        "\n",
        "data[STRG_FEATS] = data[STRG_FEATS].fillna(-100)+100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_H6pyoyA6Jc",
        "outputId": "03cdbfaf-e778-4e59-b1e0-a242fdf73847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████| 11374/11374 [02:06<00:00, 89.59it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "''' signal strength normalization (path wise) '''\n",
        "data.sort_index(inplace=True)\n",
        "tmp = []\n",
        "for _, gdf in tqdm(data.groupby('path',sort=False),\n",
        "                   ncols=60):\n",
        "    arr = gdf[STRG_FEATS].values.flatten()\n",
        "    gdf[STRG_FEATS] = (gdf[STRG_FEATS]-arr.min())/(arr.max()-arr.min())\n",
        "    tmp.append(gdf[STRG_FEATS])\n",
        "tmp = pd.concat(tmp)\n",
        "tmp.sort_index(inplace=True)\n",
        "data[STRG_FEATS] = tmp.values\n",
        "del arr,tmp; gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nAIKZTr_KxeT"
      },
      "outputs": [],
      "source": [
        " def replace_outlier(series, thresh=0.25, bias=1.5):\n",
        "    q1 = series.quantile(thresh)\n",
        "    q3 = series.quantile(1-thresh)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    outlier_min = q1 - (iqr) * bias\n",
        "    outlier_max = q3 + (iqr) * bias\n",
        "\n",
        "    series = series.clip(outlier_min, outlier_max)\n",
        "    return series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15_3RrwhYMPQ",
        "outputId": "c0cb48b3-4493-4a41-dc4d-12f12555d210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "''' IMU data standardization '''\n",
        "data[['acce_z_mean','acce_z_max','acce_z_min']] = \\\n",
        "    data[['acce_z_mean','acce_z_max','acce_z_min']]-9.80665\n",
        "for sens in['gyro','acce','ahrs','magn']:\n",
        "    ss = StandardScaler()\n",
        "    if sens=='magn':\n",
        "        tmpFEATS = [sens+'_z_mean']\n",
        "        ss.fit(data[tmpFEATS].values.reshape(-1,1))\n",
        "    else:\n",
        "        tmpFEATS = [sens+'_x_mean', sens+'_y_mean', sens+'_z_mean']\n",
        "        ss.fit(np.tile(data[tmpFEATS].values.reshape(-1,1),(1,3)))\n",
        "    data[tmpFEATS] = ss.transform(data[tmpFEATS])\n",
        "    for itm in tmpFEATS:\n",
        "        data[itm]=replace_outlier(data[itm])\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    if sens=='magn':\n",
        "        tmpFEATS = [sens+'_z_std']\n",
        "        ss.fit(data[tmpFEATS].values.reshape(-1,1))\n",
        "    else:\n",
        "        tmpFEATS = [sens+'_x_std', sens+'_y_std', sens+'_z_std']\n",
        "        ss.fit(np.tile(data[tmpFEATS].values.reshape(-1,1),(1,3)))\n",
        "    data[tmpFEATS] = ss.transform(data[tmpFEATS])\n",
        "    for itm in tmpFEATS:\n",
        "        data[itm]=replace_outlier(data[itm])\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    if sens=='magn':\n",
        "        tmpFEATS = [sens+'_z_skew']\n",
        "        ss.fit(data[tmpFEATS].values.reshape(-1,1))\n",
        "    else:\n",
        "        tmpFEATS = [sens+'_x_skew', sens+'_y_skew', sens+'_z_skew']\n",
        "        ss.fit(np.tile(data[tmpFEATS].values.reshape(-1,1),(1,3)))\n",
        "    data[tmpFEATS] = ss.transform(data[tmpFEATS])\n",
        "    for itm in tmpFEATS:\n",
        "        data[itm]=replace_outlier(data[itm])\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    if sens=='magn':\n",
        "        tmpFEATS = [sens+'_z_max', sens+'_z_min']\n",
        "        ss.fit(np.tile(data[tmpFEATS].values.reshape(-1,1),(1,2)))\n",
        "    else:\n",
        "        tmpFEATS = [sens+'_x_max', sens+'_x_min',\n",
        "                    sens+'_y_max', sens+'_y_min',\n",
        "                    sens+'_z_max', sens+'_z_min']\n",
        "        ss.fit(np.tile(data[tmpFEATS].values.reshape(-1,1),(1,6)))\n",
        "    data[tmpFEATS] = ss.transform(data[tmpFEATS])\n",
        "    for itm in tmpFEATS:\n",
        "        data[itm]=replace_outlier(data[itm])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akkdg7uqYTI9",
        "outputId": "5f338ad6-00f5-46dd-85c3-9a57aeabfda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████| 11441/11441 [00:25<00:00, 450.27it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "''' relative position '''\n",
        "DELTA_FEATS = ['delta_x_hat', 'delta_y_hat',\n",
        "               'delta_x_mag', 'delta_y_mag']\n",
        "tmp = []\n",
        "for gid, grp in tqdm(data.groupby('seq_label'), ncols=60):\n",
        "    grp[DELTA_FEATS[0]] = grp['rel_x'].diff().fillna(0)\n",
        "    grp[DELTA_FEATS[1]] = grp['rel_y'].diff().fillna(0)\n",
        "    tmp.append(grp[DELTA_FEATS[0:2]])\n",
        "assert (pd.concat(tmp).index==data.index).all()\n",
        "data[DELTA_FEATS[0:2]] = pd.concat(tmp)\n",
        "\n",
        "delta_l = np.sqrt(np.square(data[DELTA_FEATS[0:2]]).sum(axis=1))\n",
        "data[DELTA_FEATS[2]] = delta_l * data['head_magn_x_mean']\n",
        "data[DELTA_FEATS[3]] = delta_l * data['head_magn_y_mean']\n",
        "\n",
        "del tmp, delta_l; gc.collect()\n",
        "ss = StandardScaler()\n",
        "ss.fit(np.tile(data[DELTA_FEATS].values.reshape(-1,1),(1,4)))\n",
        "data[DELTA_FEATS] = ss.transform(data[DELTA_FEATS])\n",
        "for itm in DELTA_FEATS:\n",
        "    data[itm]=replace_outlier(data[itm])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V-uSC62Yf5d",
        "outputId": "c969b3c9-1839-4190-e144-79e4fcc6e24a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "''' DataFrame split '''\n",
        "trvl_data = data[data['train/test']=='train'].copy()\n",
        "test_data = data[data['train/test']=='test'].copy()\n",
        "del data; gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUs21yrQYkhc",
        "outputId": "b56715a7-8179-4fbf-a4d9-36536bb1e425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10805, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "''' train val split '''\n",
        "grb = trvl_data[['seq_label','site','floor']].groupby('seq_label')\n",
        "df_stra = grb.first()\n",
        "df_stra['seq_len'] = grb['seq_label'].count().values\n",
        "df_stra['seq_len_cut'] = pd.qcut(df_stra['seq_len'],4)\n",
        "df_stra['stratify'] = df_stra.apply(lambda x:\n",
        "                                    str(x['seq_len_cut'])+'_'+\n",
        "                                    str(x['site'])+'_'+\n",
        "                                    str(x['floor']),\n",
        "                                    axis=1)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True,\n",
        "                      random_state=SEED)\n",
        "for i, (_, val_idx) in enumerate(skf.split(df_stra['stratify'],\n",
        "                                           df_stra['stratify'])):\n",
        "    tmp = np.zeros(df_stra.shape[0]).astype(bool)\n",
        "    tmp[val_idx] = True\n",
        "    df_stra[f'fold_{i}'] = tmp\n",
        "df_stra.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHGPHNFGY0yO",
        "outputId": "2730cab5-2d14-46a6-f494-b758fb1c3282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAw0IRUyZNy_",
        "outputId": "d1db5fd4-020f-472b-b9d2-0c7ed0f26cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████| 10805/10805 [00:39<00:00, 271.92it/s]\n"
          ]
        }
      ],
      "source": [
        "''' create tensor for train data '''\n",
        "tr_id, tr_strg, tr_dlt, tr_imu, tr_site, tr_pos, tr_xy, tr_mask = \\\n",
        "    [],[],[],[],[],[],[],[]\n",
        "for gid, grp in tqdm(trvl_data.groupby('seq_label'), ncols=60):\n",
        "    tr_id.append(torch.from_numpy(grp[ID_FEATS].values).long())\n",
        "    tr_strg.append(torch.from_numpy(grp[STRG_FEATS].values).float())\n",
        "    tr_dlt.append(torch.from_numpy(grp[DELTA_FEATS].values).float())\n",
        "    tr_imu.append(torch.from_numpy(grp[IMU_FEATS].values).float())\n",
        "    tr_site.append(torch.from_numpy(grp['site'].values).long())\n",
        "    tr_pos.append(torch.from_numpy(grp['len_pos'].values).float())\n",
        "    tr_xy.append(torch.from_numpy(grp[['x','y']].values).float())\n",
        "    ''' tr_mask: the tensor for padding '''\n",
        "    tr_mask.append(tr_id[-1].sum(dim=-1)==0)\n",
        "tr_id = rnn.pad_sequence(tr_id, batch_first=True).to(device)\n",
        "tr_strg = rnn.pad_sequence(tr_strg, batch_first=True).to(device)\n",
        "tr_dlt = rnn.pad_sequence(tr_dlt, batch_first=True).to(device)\n",
        "tr_imu = rnn.pad_sequence(tr_imu, batch_first=True).to(device)\n",
        "tr_site = rnn.pad_sequence(tr_site, batch_first=True).to(device)\n",
        "tr_pos = rnn.pad_sequence(tr_pos, batch_first=True).to(device)\n",
        "tr_xy = rnn.pad_sequence(tr_xy, batch_first=True).to(device)\n",
        "tr_mask = rnn.pad_sequence(tr_mask, batch_first=True,\n",
        "                           padding_value=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "O2S1gWyZZSvD"
      },
      "outputs": [],
      "source": [
        "class ILNdatasets(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_id, x_strg, x_dlt, x_imu, x_site, x_pos, x_mask,\n",
        "                 y, train=True):\n",
        "        self.train = train\n",
        "        self.x_id = x_id\n",
        "        self.x_strg = x_strg\n",
        "        self.x_dlt = x_dlt\n",
        "        self.x_imu = x_imu\n",
        "        self.x_site = x_site\n",
        "        self.x_pos = x_pos\n",
        "        self.x_mask = x_mask\n",
        "        self.y = y\n",
        "        self.datanum = len(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datanum\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x_id[idx], self.x_strg[idx], self.x_dlt[idx], self.x_imu[idx],\n",
        "                self.x_site[idx], self.x_pos[idx], self.x_mask[idx],\n",
        "                self.y[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nBhSvxZ-Zh2u"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    def forward(self, x, position, mask):\n",
        "        if position is None:\n",
        "            position = torch.cumsum(mask.logical_not(), dim=1)-1\n",
        "        else:\n",
        "            position *= mask.logical_not()\n",
        "        position = rearrange(position, 'bn seq -> bn seq 1')\n",
        "\n",
        "        pe = torch.zeros(x.shape).to(x.device)\n",
        "        div_term = self.div_term.to(x.device)\n",
        "        pe[:,:,0::2] = torch.sin(position * div_term)\n",
        "        pe[:,:,1::2] = torch.cos(position * div_term)\n",
        "        x = x + pe\n",
        "\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ez5aOHvZZm6V"
      },
      "outputs": [],
      "source": [
        "''' Special Thanks to\n",
        "    https://www.kaggle.com/shinomoriaoshi/iln-transformer-train-k1?scriptVersionId=59645688 '''\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "def attention(query, key, value, key_padding_mask=None, attn_weight=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if key_padding_mask is not None:\n",
        "        scores = scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), -1e9)\n",
        "    if attn_weight is not None:\n",
        "        if len(attn_weight.shape) != len(scores.shape):\n",
        "            attn_weight = attn_weight.unsqueeze(-3)\n",
        "        scores -= attn_weight\n",
        "    p_attn = nn.functional.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % nhead == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // nhead\n",
        "        self.nhead = nhead\n",
        "        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, attn_weight=None):\n",
        "        \"Implements Figure 2\"\n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.nhead, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(query, key, value, key_padding_mask=key_padding_mask,\n",
        "                                 attn_weight=attn_weight,\n",
        "                                 dropout=self.dropout)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(nbatches, -1, self.nhead * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(nn.functional.relu(self.w_1(x))))\n",
        "\n",
        "class CustomEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Encoder block of SAINT\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, nhead, dim_feedforward = 1024, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self._self_attn = MultiHeadedAttention(d_model, nhead, dropout)\n",
        "        self._ffn = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n",
        "        self._layernorms = clones(nn.LayerNorm(d_model, eps=1e-6), 2)\n",
        "        self._dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, key_padding_mask = None, attn_weight = None):\n",
        "        \"\"\"\n",
        "        query: question embeddings\n",
        "        key: interaction embeddings\n",
        "        \"\"\"\n",
        "        # self-attention block\n",
        "        src2 = self._self_attn(query=src, key=src, value=src, key_padding_mask=key_padding_mask,\n",
        "                               attn_weight=attn_weight)\n",
        "        src = src + self._dropout(src2)\n",
        "        src = self._layernorms[0](src)\n",
        "        src2 = self._ffn(src)\n",
        "        src = src + self._dropout(src2)\n",
        "        src = self._layernorms[1](src)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bybifrOXZuZ7"
      },
      "outputs": [],
      "source": [
        "''' Learning Model '''\n",
        "class ILNnet(nn.Module):\n",
        "    def __init__(self, input_dim, id_num_embd, id_embd_dim,\n",
        "                 strg_dim, site_num_embd, imu_dim,\n",
        "                 ninp, nhead, nhid, nlayers, dropout):\n",
        "        super(ILNnet, self).__init__()\n",
        "\n",
        "        ''' embedding and concat '''\n",
        "        self.id_embd = nn.Embedding(id_num_embd, id_embd_dim,\n",
        "                                    padding_idx=0)\n",
        "        self.strg_ln = nn.LayerNorm(input_dim)\n",
        "        self.strg_lin = nn.Linear(input_dim, strg_dim)\n",
        "        self.site_embd = nn.Embedding(site_num_embd, ninp)\n",
        "        cat_dim = input_dim*id_embd_dim + strg_dim\n",
        "        self.main_seq1 = nn.Sequential(nn.Linear(cat_dim, ninp),\n",
        "                                       nn.LayerNorm(ninp),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(dropout))\n",
        "        self.dltimu_lin = nn.Linear(4+imu_dim, ninp)\n",
        "\n",
        "        ''' main stream(1) TRANSFORMER with positional decay '''\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.attn_wt_coef = nn.Parameter(torch.clamp(torch.randn(nhead), min=1e-3))\n",
        "        self.customTR1 = CustomEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.customTR2 = CustomEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "\n",
        "        ''' main stream(2) LSTM '''\n",
        "        self.lstm1 = nn.LSTM(batch_first=True,\n",
        "                             bidirectional=True, num_layers=1,\n",
        "                             input_size=ninp, hidden_size=ninp//2)\n",
        "        self.lstm2 = nn.LSTM(batch_first=True,\n",
        "                             bidirectional=True, num_layers=1,\n",
        "                             input_size=ninp, hidden_size=ninp//2)\n",
        "        # self.lstm3 = nn.LSTM(batch_first=True,\n",
        "        #                      bidirectional=True, num_layers=1,\n",
        "        #                      input_size=ninp, hidden_size=ninp//2)\n",
        "        self.main_seq2 = nn.Sequential(nn.Linear(ninp,ninp),\n",
        "                                       nn.LayerNorm(ninp),\n",
        "                                       nn.Dropout(dropout),\n",
        "                                       nn.Linear(ninp,2))\n",
        "        self.init_weights()\n",
        "        # self.correct = CorrectWithDelta()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.id_embd.weight.data.zero_()\n",
        "        self.site_embd.weight.data.zero_()\n",
        "\n",
        "    def forward(self, x_id, x_strg, x_dlt, x_imu, x_site, x_pos, x_mask):\n",
        "\n",
        "        ''' embedding and concat '''\n",
        "        x_id = self.id_embd(x_id)\n",
        "        x_id = rearrange(x_id, 'bn seq d1 d2 -> bn seq (d1 d2)')\n",
        "        x_strg = self.strg_ln(x_strg)\n",
        "        x_strg = torch.relu(self.strg_lin(x_strg))\n",
        "        x_site = self.site_embd(x_site)\n",
        "        x = torch.cat((x_id, x_strg), dim=-1)\n",
        "        x = self.main_seq1(x) + x_site\n",
        "\n",
        "        ''' concat delta & IMU '''\n",
        "        x_dltimu = torch.cat((x_dlt, x_imu),-1)\n",
        "        x_dltimu = torch.relu(self.dltimu_lin(x_dltimu))\n",
        "        x = x + x_dltimu\n",
        "\n",
        "        ''' main stream(1) TRANSFORMER with positional decay '''\n",
        "        xt = self.pos_encoder(x, None, x_mask)\n",
        "        attn_wt = (rearrange(x_pos, 'bn seq -> bn 1 seq 1')\n",
        "                  -rearrange(x_pos, 'bn seq -> bn 1 1 seq')).abs()\n",
        "        attn_coef = rearrange(self.attn_wt_coef, 'nh -> 1 nh 1 1')\n",
        "        xt = self.customTR1(xt, x_mask, torch.log(1.0 + attn_wt) * attn_coef)\n",
        "        xt = self.customTR2(xt, x_mask, torch.log(1.0 + attn_wt) * attn_coef)\n",
        "        x = x + xt\n",
        "\n",
        "        ''' main stream(2) LSTM '''\n",
        "        lengths = x_mask.logical_not().sum(dim=1).to('cpu')\n",
        "        x = rnn.pack_padded_sequence(x, lengths, batch_first=True,\n",
        "                                     enforce_sorted=False)\n",
        "        x,_ = self.lstm1(x)\n",
        "        x,_ = self.lstm2(x)\n",
        "        # x,_ = self.lstm3(x)\n",
        "        x = rnn.pad_packed_sequence(x, batch_first=True)[0]\n",
        "        if x.size(1)!=x_mask.size(1):\n",
        "            pad_len = x_mask.size(1)-x.size(1)\n",
        "            x = torch.cat([x, torch.zeros(x.size(0),pad_len,x.size(2)).to(device)],\n",
        "                          dim=1)\n",
        "            assert x.size(1)==x_mask.size(1)\n",
        "        x = self.main_seq2(x)\n",
        "\n",
        "        # ''' correct with delta '''\n",
        "        # c = torch.relu(x[:,:,2])\n",
        "        # x = x[:,:,0:2]\n",
        "        # x = self.correct(x, c, x_dlt, x_pos, x_mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "p141uZWMZ2ZQ"
      },
      "outputs": [],
      "source": [
        "class LearningRateScheduler:\n",
        "    def __init__(self, lr:list,\n",
        "                 switch_epoch: list):\n",
        "        self.lr = lr\n",
        "        self.switch_epoch = switch_epoch\n",
        "\n",
        "    def __call__(self, epoch:int):\n",
        "        idx = [i>epoch for i\n",
        "               in self.switch_epoch+[1e9]].index(True)\n",
        "        return self.lr[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UxZFsXtuZ5zX"
      },
      "outputs": [],
      "source": [
        "''' This function plays the same role as model.summary() of keras '''\n",
        "def param_count(model, print_all=False):\n",
        "    if print_all:\n",
        "        print('-'*80)\n",
        "    psum=0\n",
        "    for n,p in model.named_parameters():\n",
        "        if p.requires_grad:\n",
        "            if print_all:\n",
        "                print(f'{n}:')\n",
        "                print(f'     params:{p.numel():,},  {p.shape}')\n",
        "#                 print(p)\n",
        "            psum += p.numel()\n",
        "    print(f'Total params: {psum:,}')\n",
        "    if print_all:\n",
        "        print('-'*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "IVK-6dL_Z_DE"
      },
      "outputs": [],
      "source": [
        "def train_val(net, trainloader, valloader, bsz):\n",
        "    ''' train '''\n",
        "    loss_sum, metric_sum = 0., 0.\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "    # note: \"prefix\" and \"descpost\" are the variable just for tqdm\n",
        "    with tqdm(trainloader, ncols=80) as pbar:\n",
        "        prefix = f\"epoch {i_epoch+1} train\"\n",
        "        pbar.set_description(prefix)\n",
        "        descpost = None\n",
        "        for i, (tr_id, tr_strg, tr_dlt, tr_imu, tr_site, tr_pos,\n",
        "                tr_mask, tr_xy) in enumerate(pbar):\n",
        "\n",
        "            output = net(tr_id, tr_strg, tr_dlt, tr_imu, tr_site, tr_pos, tr_mask)\n",
        "            filter_ = tr_mask.logical_not()\n",
        "            output *= filter_.unsqueeze(-1)\n",
        "\n",
        "            loss, metric = ILN_loss(output, filter_, tr_dlt, tr_xy)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            metric_sum += metric.item()\n",
        "            descpost = f'{loss_sum/(i+1):.2f}/{metric_sum/(i+1):.2f}'\n",
        "            pbar.set_postfix({\"l/m\":descpost})\n",
        "        history['epoch'].append(i_epoch+1)\n",
        "        history['train_loss'].append(loss_sum/(i+1))\n",
        "        history['train_metric'].append(metric_sum/(i+1))\n",
        "\n",
        "    ''' validation '''\n",
        "    with torch.no_grad():\n",
        "        loss_sum, metric_sum = 0., 0.\n",
        "        net.eval()\n",
        "        with tqdm(valloader, ncols=80) as pbar:\n",
        "            prefix = ' '*(len(prefix)-10)+'validation'\n",
        "            pbar.set_description(prefix)\n",
        "            descpost = None\n",
        "            for i, (vl_id, vl_strg, vl_dlt, vl_imu, vl_site, vl_pos, vl_mask, vl_xy)\\\n",
        "              in enumerate(pbar):\n",
        "\n",
        "                output = net(vl_id, vl_strg, vl_dlt, vl_imu, vl_site, vl_pos, vl_mask)\n",
        "                filter_ = vl_mask.logical_not()\n",
        "                output *= filter_.unsqueeze(-1)\n",
        "\n",
        "                loss, metric = ILN_loss(output, filter_, vl_dlt, vl_xy)\n",
        "                loss_sum += loss.item()\n",
        "                metric_sum += metric.item()\n",
        "                descpost = f'{loss_sum/(i+1):.2f}/{metric_sum/(i+1):.2f}'\n",
        "                pbar.set_postfix({\"l/m\":descpost})\n",
        "    history['val_loss'].append(loss_sum/(i+1))\n",
        "    history['val_metric'].append(metric_sum/(i+1))\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    return net, history['val_metric'][-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2zAekrxnozAc"
      },
      "outputs": [],
      "source": [
        "inference_only=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MWDNvrL1pNpc"
      },
      "outputs": [],
      "source": [
        "def ILN_loss(output, filter_, delta, label):\n",
        "\n",
        "    metr1 = (output[:,:,:2]-label[:,:,:2]).square()\n",
        "    metr1 = metr1.sum(dim=-1)+1e-6\n",
        "    metr1 = metr1.sqrt().sum()\n",
        "\n",
        "    # metr2 = ((output[:,1:,:2]-output[:,:-1,:2])\n",
        "    #          -delta[:,1:,:])**2\n",
        "    # metr2 = metr2.sum(dim=-1)+1e-6\n",
        "    # metr2 = (metr2.sqrt()*filter_[:,1:]).sum()\n",
        "\n",
        "    # metr3 = ((output[:,1:,:2]-output[:,:-1,:2]).square().sum(dim=-1)+1e-6).sqrt() \\\n",
        "    #         -(delta[:,1:,:].square().sum(dim=-1)+1e-6).sqrt()\n",
        "    # metr3 = (metr3 * filter_[:,1:]).abs().sum()\n",
        "\n",
        "    # loss = (metr1 + 10.0 * metr3)/filter_.sum()\n",
        "    metric = metr1/filter_.sum()\n",
        "    loss = metric\n",
        "\n",
        "    return loss, metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CAkU4fj-x97r"
      },
      "outputs": [],
      "source": [
        "inference_only=False"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyaYjL4Pst_K",
        "outputId": "dd492400-8e7a-480d-b419-cd4631173bf8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings dimension for wifi128\n",
            "Wifi signal out dimensions1280\n",
            "Enter number of attention heads2\n",
            "Enter number of neurons for fully connected transformer layers512\n",
            "dropout percentage0.2\n",
            "Number of Epochs2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iiBPdCHaI5n",
        "outputId": "b8c49db8-1216-4fea-be70-6506fb16de09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== FOLD 0 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 train: 100%|█████████| 76/76 [00:09<00:00,  8.40it/s, l/m=159.31/159.31]\n",
            "   validation: 100%|███████████| 9/9 [00:00<00:00, 23.38it/s, l/m=156.27/156.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================== FOLD 1 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 train: 100%|█████████| 76/76 [00:08<00:00,  8.45it/s, l/m=158.97/158.97]\n",
            "   validation: 100%|███████████| 9/9 [00:00<00:00, 23.40it/s, l/m=154.45/154.45]\n"
          ]
        }
      ],
      "source": [
        "if not inference_only:\n",
        "    for i_fold in range(int(Epochs)):\n",
        "        print('='*20 + f' FOLD {i_fold} ' + '='*20)\n",
        "\n",
        "        ''' model comopile '''\n",
        "        net = ILNnet(NUM_FEATS, len(ID_vocab)+1, wifi_embd, wifi_out_dimension, 24, len(IMU_FEATS),\n",
        "                     ninp=256, nhead=nhead, nhid=nhid, nlayers=2, dropout=dropout)\n",
        "        net = net.to(device)\n",
        "    #     param_count(net, print_all=True)\n",
        "\n",
        "        mask = df_stra[f'fold_{i_fold}'].values\n",
        "    #     trn_idx = np.arange(df_stra.shape[0]) # train with all data\n",
        "        trn_idx = np.where(~mask)[0]\n",
        "        val_idx = np.where(mask)[0]\n",
        "\n",
        "        trainvaldata = ILNdatasets(tr_id, tr_strg, tr_dlt, tr_imu, tr_site,\n",
        "                                   tr_pos, tr_mask, tr_xy)\n",
        "        traindata = torch.utils.data.dataset.Subset(trainvaldata,\n",
        "                                                    trn_idx)\n",
        "        valdata = torch.utils.data.dataset.Subset(trainvaldata,\n",
        "                                                  val_idx)\n",
        "        trainloader = torch.utils.data.DataLoader(traindata,\n",
        "                                                  # pin_memory=True,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "        valloader = torch.utils.data.DataLoader(valdata,\n",
        "                                                # pin_memory=True,\n",
        "                                                shuffle=True,\n",
        "                                                batch_size=batch_size)\n",
        "\n",
        "        optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
        "        lr_scheduler_func = LearningRateScheduler([1.0, 0.1],\n",
        "                                                  [int(0.75*n_epoch)])\n",
        "        lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
        "                           lr_lambda=lr_scheduler_func)\n",
        "\n",
        "        history = {'epoch':[], 'train_loss': [], 'val_loss': [],\n",
        "                   'train_metric': [], 'val_metric': []}\n",
        "\n",
        "        ''' train and validation '''\n",
        "        time.sleep(1)\n",
        "        early_stop = 0\n",
        "        min_metric = 1000.\n",
        "        for i_epoch in range(1):\n",
        "            net, es_metric = train_val(net, trainloader, valloader, batch_size)\n",
        "\n",
        "            ''' history json dump '''\n",
        "            with open(f'_history_{model_ver}_fold{i_fold}_latest.json', 'w') as f:\n",
        "                json.dump(history, f, indent=4)\n",
        "\n",
        "    #         ''' early stopping '''\n",
        "    #         es_metric = round(es_metric,2)\n",
        "    #         if es_metric<min_metric:\n",
        "    #             min_metric=es_metric\n",
        "    #             early_stop=0\n",
        "    #         else:\n",
        "    #             early_stop+=1\n",
        "    #         if early_stop>10 and i_epoch>int(n_epoch*0.5):\n",
        "    #             print('early stopping.')\n",
        "    #             break\n",
        "\n",
        "        i_epoch += 1\n",
        "\n",
        "        ''' model output '''\n",
        "        model_path = f'final_model.pth'\n",
        "        torch.save(net.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dCWciBDwKOpz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91aaa6e2-34ec-49aa-e2ff-7867f863dbb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_history_ILN_632_fold0_latest.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "f'_history_{model_ver}_fold{i_fold}_latest.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "qJcnJ_8BaUVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e596d313-bb2b-4e99-871c-7ff91893318d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████| 636/636 [00:03<00:00, 168.63it/s]\n"
          ]
        }
      ],
      "source": [
        "''' create tensor for test data '''\n",
        "te_id, te_strg, te_dlt, te_imu, te_site, te_pos, te_mask = \\\n",
        "    [],[],[],[],[],[],[]\n",
        "for gid, grp in tqdm(test_data.groupby('seq_label'), ncols=60):\n",
        "    te_id.append(torch.from_numpy(grp[ID_FEATS].values).long())\n",
        "    te_strg.append(torch.from_numpy(grp[STRG_FEATS].values).float())\n",
        "    te_dlt.append(torch.from_numpy(grp[DELTA_FEATS].values).float())\n",
        "    te_imu.append(torch.from_numpy(grp[IMU_FEATS].values).float())\n",
        "    te_site.append(torch.from_numpy(grp['site'].values).long())\n",
        "    te_pos.append(torch.from_numpy(grp['len_pos'].values).float())\n",
        "    ''' te_mask: the tensor for padding '''\n",
        "    te_mask.append(te_id[-1].sum(dim=-1)==0)\n",
        "te_id = rnn.pad_sequence(te_id, batch_first=True).to(device)\n",
        "te_strg = rnn.pad_sequence(te_strg, batch_first=True).to(device)\n",
        "te_dlt = rnn.pad_sequence(te_dlt, batch_first=True).to(device)\n",
        "te_imu = rnn.pad_sequence(te_imu, batch_first=True).to(device)\n",
        "te_site = rnn.pad_sequence(te_site, batch_first=True).to(device)\n",
        "te_pos = rnn.pad_sequence(te_pos, batch_first=True).to(device)\n",
        "te_mask = rnn.pad_sequence(te_mask, batch_first=True,\n",
        "                           padding_value=True).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3U8BirQsJGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Bhj6OEryls1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c3cf8f-d5d7-458e-a08b-322307b08eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████| 5/5 [00:00<00:00, 18.37it/s]\n",
            "100%|████████████████████| 626/626 [00:02<00:00, 284.22it/s]\n"
          ]
        }
      ],
      "source": [
        "test_data.set_index('site_path_timestamp', inplace=True)\n",
        "\n",
        "''' model comopile '''\n",
        "net = ILNnet(NUM_FEATS, len(ID_vocab)+1, 128, 1280, 24, len(IMU_FEATS),\n",
        "             ninp=256, nhead=2, nhid=512, nlayers=2, dropout=0.1)\n",
        "net = net.to(device)\n",
        "\n",
        "''' prediction '''\n",
        "xy_all = []\n",
        "\n",
        "\n",
        "''' model load '''\n",
        "with open('_history_ILN_632_fold0_latest.json','r') as f:\n",
        "        history = json.load(f)\n",
        "i_epoch = history['epoch'][-1]\n",
        "model_path = f'final_model.pth'\n",
        "net.load_state_dict(torch.load(model_path))\n",
        "\n",
        "net.eval()\n",
        "pred = []\n",
        "with torch.no_grad():\n",
        "    for te_id_ch, te_strg_ch, te_dlt_ch, te_imu_ch, \\\n",
        "        te_site_ch, te_pos_ch, te_mask_ch in \\\n",
        "        (zip(tqdm(te_id.split(batch_size),ncols=60),\n",
        "              te_strg.split(batch_size), te_dlt.split(batch_size),\n",
        "              te_imu.split(batch_size), te_site.split(batch_size),\n",
        "              te_pos.split(batch_size), te_mask.split(batch_size))):\n",
        "        output = net(te_id_ch, te_strg_ch, te_dlt_ch, te_imu_ch, te_site_ch,\n",
        "                      te_pos_ch, te_mask_ch)\n",
        "        output  = rearrange(output, 'bn seq d -> (bn seq) d')\n",
        "        filter_ = rearrange(te_mask_ch.logical_not(),\n",
        "                            'bn seq -> (bn seq)')\n",
        "        pred.append(output[filter_])\n",
        "test_data[['x','y']] = torch.cat(pred, dim=0).to('cpu').numpy()\n",
        "test_data['floor']=0\n",
        "\n",
        "subm = pd.read_csv(subm_file, index_col=0)\n",
        "subm.loc[:,:]=np.nan\n",
        "all_preds = pd.concat([test_data[subm.columns].copy() ,subm])\n",
        "all_preds.sort_index(inplace=True)\n",
        "df_tmp = pd.Series(all_preds.index).str.split('_', expand=True)\n",
        "df_tmp.index = all_preds.index\n",
        "df_tmp.columns = ['site','path','timestamp']\n",
        "all_preds = pd.concat([all_preds, df_tmp],axis=1)\n",
        "del df_tmp; gc.collect()\n",
        "all_preds['timestamp'] = all_preds['timestamp'].astype('int')\n",
        "\n",
        "tmp = []\n",
        "for gid, gdf in tqdm(all_preds.groupby('path'),ncols=60):\n",
        "    gdf.reset_index(drop=False, inplace=True)\n",
        "    gdf.set_index('timestamp', inplace=True)\n",
        "    gdf.sort_index(inplace=True)\n",
        "#         for itm in ['x','y']:\n",
        "#             gdf[itm].interpolate('nearest', inplace=True)\n",
        "#             gdf[itm].fillna(method='bfill', inplace=True)\n",
        "#             gdf[itm].fillna(method='ffill', inplace=True)\n",
        "    gdf[['x','y']] = gdf[['x','y']].interpolate(limit_direction='both',\n",
        "                                                method='index')\n",
        "    gdf.set_index('site_path_timestamp', inplace=True)\n",
        "    tmp.append(gdf[['floor','x','y']])\n",
        "\n",
        "all_preds = pd.concat(tmp).groupby(level=0).mean().reindex(subm.index)\n",
        "\n",
        "# simple_accurate_99 = pd.read_csv('submission.csv')\n",
        "# all_preds['floor'] = simple_accurate_99['floor'].values\n",
        "\n",
        "all_preds.to_csv(f'{model_ver}_fold{i_fold}_submission.csv')\n",
        "xy_all.append(all_preds[['x','y']].values)\n",
        "\n",
        "all_preds[['x','y']] = np.stack(xy_all, axis=-1).mean(axis=-1)\n",
        "all_preds.to_csv(f'./{model_ver}_fold_all_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "04SMafADoTUG"
      },
      "outputs": [],
      "source": [
        "check_test=pd.read_csv(subm_file, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "XKz8EMHXomDu"
      },
      "outputs": [],
      "source": [
        "all_preds['floor'] = check_test['floor'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "R4hV8pM7uH6u"
      },
      "outputs": [],
      "source": [
        "all_preds.to_csv(f'./{model_ver}_fold_all_submission.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}